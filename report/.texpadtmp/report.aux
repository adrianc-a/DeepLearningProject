\relax 
\citation{eight,nine,ten}
\citation{one,two,three}
\citation{one}
\citation{two}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{AlphaGoZero}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Holistic Model}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces }}{3}}
\newlabel{fig:figure_training}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Search Algorithm of MCTS-Simulation}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Select: Each simulation traverses the tree by selecting the edge with maximum action-value $Q$, plus an upper confidence bound $U$ that depends on a stored prior probability $P$ and visit count $N$ for that edge (which is incremented once traversed). Expand \& Evaluate: The leaf node is expanded and the associated positions is evaluated by the neural network $(P(s,\cdot ),V(s))=f_{\theta }(s)$ ; the vector of $P$ values are stored in the outgoing edges from $s$. Backup: Action-values $Q$ are updated to track the mean of all evaluations $V$ in the subtree below that action. play Once the search is complete, search probabilities $\pi $ are returned, proportional to $N^{\frac  {1}{\tau }}$, where $N$ is the visit count of each move from the root state and $\tau $ is a parameter controlling temperature.}}{4}}
\newlabel{fig:figure_mcts}{{2}{4}}
\newlabel{eq:1}{{1}{4}}
\newlabel{eq:2}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Self play}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces }}{6}}
\newlabel{fig:figure_self_play}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Evaluator}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Neural Network Architecture}{6}}
\citation{AlphaGo}
\citation{Elo}
\citation{ELO}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Our network for tic-tac-toe with 5 residual blocks}}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Performance}{8}}
\newlabel{fig:lossMSE}{{3}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The modified MSE loss function compared with the original MSE and Cross-Entropy. Due to our modifications, the original loss function did not capture quality of moves.}}{9}}
\newlabel{fig:noise}{{3}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The original loss function with our modifications is very noisy.}}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The performance of the model trained for the tic-tac-toe game over time}}{10}}
\newlabel{fig:figure_ttt}{{7}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{11}}
\bibdata{references}
\bibcite{eight}{1}
\bibcite{nine}{2}
\bibcite{ten}{3}
\bibcite{one}{4}
\bibcite{two}{5}
\bibcite{three}{6}
\bibcite{AlphaGoZero}{7}
\bibcite{AlphaGo}{8}
\bibcite{Elo}{9}
\bibstyle{IEEEtran}
\@writefile{toc}{\contentsline {section}{\numberline {5}Author contributions}{12}}
